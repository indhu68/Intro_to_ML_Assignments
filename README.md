# Machine Learning Assignments
This repository contains my assignments for the Introduction to Machine Learning course. Each assignment focuses on developing machine learning and neural network models, including linear regression, logistic regression, support vector machines, neural networks, and deep learning techniques.

# Table of Contents
Homework 1 - Linear Regression
Homework 2 - Gradient Descent & Feature Scaling, Develop training and evaluation code from scratch
Homework 3 - Logistic Regression & Naive Bayes
Homework 4 - Principal Componenet Analysis(PCA), Support Vector Machines (SVM) & Support Vector Regression (SVR)
Homework 5 - Non-linear Models & Housing Price Prediction
Homework 6 - Fully Connected Neural Network
Homework 7 - Convolutional Neural Networks & ResNet



**Homework 1 - Linear Regression**

-> Implemented linear regression using gradient descent from scratch.
-> Explored different learning rates and their impact on convergence.
-> Compared individual variables' impact on the output (cost function analysis).
-> **Final Observations:** A moderate learning rate achieved the best balance between accuracy and convergence speed.


**Homework 2 - Gradient Descent & Feature Scaling, Develop training and evaluation code from scratch**

-> Developed a gradient decent training and evaluation code, from scratch, which predicts housing price based on the  input variables provided.
-> Implemented gradient descent for predicting housing prices.
-> Explored different learning rates and their effect on training.
-> Compared standardization vs. normalization for input scaling.
-> **Final Observations:** Normalization improved convergence and accuracy, while feature scaling helped in balancing coefficient impacts.


**Homework 3 - Logistic Regression & Naive Bayes**

-> Built a logistic regression classifier for diabetes prediction.
-> Trained a logistic regression model for cancer detection.
-> Implemented Naïve Bayes for cancer classification and compared its performance with logistic regression.
-> **Final Observations:** Logistic regression outperformed Naïve Bayes, achieving higher precision, recall, and F1-score.



**Homework 4 - Principal Componenet Analysis(PCA), Support Vector Machines (SVM) & Support Vector Regression (SVR)**

-> Built an SVM classifier for cancer classification.
-> Applied Principal Component Analysis (PCA) to optimize the number of features.
-> Compared different SVM kernels (Linear, RBF, Polynomial) for performance analysis.
-> Developed a Support Vector Regression (SVR) model for predicting housing prices.
-> **Final Observations:** SVM with an RBF kernel had the highest accuracy (99.12%). SVR performed worse than linear regression with regularization.


**Homework 5 - Non-linear Models & Housing Price Prediction**

-> Developed a non-linear regression model for temperature prediction.
-> Implemented SGD and ADAM optimizers for training.
-> Trained a linear regression model for housing price prediction and compared its performance with previous linear models.
-> **Final Observations:** The Adam optimizer with a learning rate of 0.001 achieved the best performance.


**Homework 6 - Fully Connecetd Neural Network**

-> Built a Fully connected neural network for classification.
-> Compared training and validation losses against a previous linear regression model.
-> **Final Observations:** A 3-layer neural network had better training performance but slightly worse validation accuracy compared to a single-layer model.


**Homework 7 - Convolutional Neural Networks & ResNet**

-> Built a CNN for image classification using the CIFAR-10 dataset.
-> Compared CNN vs. fully connected neural networks (FCN).
-> Extended the CNN with an additional convolution and pooling layer.
-> Implemented ResNet-10 with skip connections.
-> Applied regularization techniques (Weight Decay, Dropout, Batch Normalization) to evaluate performance improvements.
-> **Final Observations:**
    -> ResNet-10 achieved the best accuracy (66%).
    -> Weight decay improved training time but slightly increased training loss.
    -> Dropout and batch normalization improved generalization.



Author
Indhuja Gudluru
Masters in Computer Engineering
